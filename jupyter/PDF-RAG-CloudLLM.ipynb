{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fah5abnsj",
   "metadata": {},
   "source": [
    "# PDF Question Answering with LLM and InterSystems IRIS\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using:\n",
    "- **InterSystems IRIS** as the multimodel database (relational and vector in this example)\n",
    "- **Sentence Transformers** for text embeddings\n",
    "- **Mistral AI** as the Large Language Model\n",
    "- **LangChain** for orchestrating the RAG pipeline\n",
    "\n",
    "## Workshop Overview\n",
    "We'll process PDF documents, store them as vectors in IRIS, and enable natural language querying with accurate, contextual answers.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import all required libraries for our RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f7b5878-d158-4423-97d3-9baf3b4cfe1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Mistral API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import irisnative          # InterSystems IRIS native database connection\n",
    "import os                  # Operating system interface\n",
    "import getpass             # Secure password input\n",
    "from pypdf import PdfReader                        # PDF reading capability\n",
    "import sentence_transformers                       # Text embedding models\n",
    "import numpy as np                                 # Numerical computations\n",
    "\n",
    "# LangChain components for RAG pipeline\n",
    "from langchain import hub                          # Pre-built prompts from LangChain Hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Text chunking\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader # PDF loading\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.runnables import RunnablePassthrough              # Pipeline utilities\n",
    "from langchain_core.output_parsers import StrOutputParser             # Output parsing\n",
    "\n",
    "# Set up Mistral AI API key securely\n",
    "# This prompts the user to enter their API key without displaying it on screen\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")\n",
    "\n",
    "# Import Mistral AI LLM after setting the API key\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvg0yufbnd9",
   "metadata": {},
   "source": [
    "## 2. Database and LLM Initialization\n",
    "\n",
    "Now we'll establish connections to our InterSystems IRIS database and initialize our Large Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915cd2cb-059f-40b1-86b3-7ead09269b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to InterSystems IRIS database\n",
      "‚úÖ Mistral AI LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# Database connection parameters\n",
    "# These should match your InterSystems IRIS instance configuration\n",
    "connection_string = \"iris:1972/LLMRAG\"  # host:port/namespace\n",
    "username = \"superuser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# Establish connection to InterSystems IRIS database\n",
    "# This creates both a connection and a cursor for executing SQL commands\n",
    "connectionIRIS = irisnative.createConnection(connection_string, username, password)\n",
    "cursorIRIS = connectionIRIS.cursor()\n",
    "print(\"‚úÖ Connected to InterSystems IRIS database\")\n",
    "\n",
    "# Initialize the Mistral AI Large Language Model\n",
    "# mistral-large-latest is their most capable model for complex reasoning tasks\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "print(\"‚úÖ Mistral AI LLM initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2br7jwhvsm6",
   "metadata": {},
   "source": [
    "## 3. Embedding Model Setup\n",
    "\n",
    "We need a sentence transformer model to convert text into numerical vectors (embeddings). This allows us to perform semantic similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3961787a-327b-4f8a-bd8e-a929ee686eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model already available locally\n"
     ]
    }
   ],
   "source": [
    "# Check if the embedding model is already downloaded and saved locally\n",
    "# This saves time and bandwidth by avoiding re-downloads\n",
    "if not os.path.isdir('/app/data/model/'):\n",
    "    print(\"üì• Downloading and saving embedding model...\")\n",
    "    # paraphrase-multilingual-MiniLM-L12-v2 is excellent for multilingual semantic similarity\n",
    "    # It's lightweight but effective for most RAG applications\n",
    "    model = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    model.save('/app/data/model/')\n",
    "    print(\"‚úÖ Model saved to local directory\")\n",
    "else:\n",
    "    print(\"‚úÖ Embedding model already available locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dxth29iojlv",
   "metadata": {},
   "source": [
    "## 4. Document Processing and Vector Storage\n",
    "\n",
    "This is the core of our RAG system: we'll load PDF documents, split them into chunks, create embeddings, and store everything in InterSystems IRIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944333c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading PDF documents...\n",
      "‚úÖ Loaded 51 document pages\n",
      "‚úÇÔ∏è Splitting documents into chunks...\n",
      "‚úÖ Created 239 text chunks\n",
      "üîÑ Loading embedding model...\n",
      "‚úÖ Embedding model loaded\n",
      "üíæ Processing chunks and storing in database...\n",
      "  Processed 10/239 chunks\n",
      "  Processed 20/239 chunks\n",
      "  Processed 30/239 chunks\n",
      "  Processed 40/239 chunks\n",
      "  Processed 50/239 chunks\n",
      "  Processed 60/239 chunks\n",
      "  Processed 70/239 chunks\n",
      "  Processed 80/239 chunks\n",
      "  Processed 90/239 chunks\n",
      "  Processed 100/239 chunks\n",
      "  Processed 110/239 chunks\n",
      "  Processed 120/239 chunks\n",
      "  Processed 130/239 chunks\n",
      "  Processed 140/239 chunks\n",
      "  Processed 150/239 chunks\n",
      "  Processed 160/239 chunks\n",
      "  Processed 170/239 chunks\n",
      "  Processed 180/239 chunks\n",
      "  Processed 190/239 chunks\n",
      "  Processed 200/239 chunks\n",
      "  Processed 210/239 chunks\n",
      "  Processed 220/239 chunks\n",
      "  Processed 230/239 chunks\n",
      "‚úÖ Successfully stored 239 chunks in InterSystems IRIS database\n"
     ]
    }
   ],
   "source": [
    "# Configure text splitting strategy\n",
    "# Smaller chunks (700 chars) ensure focused, relevant context retrieval\n",
    "# Overlap (50 chars) prevents important information from being split across chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,      # Maximum characters per chunk\n",
    "    chunk_overlap=50,    # Characters to overlap between adjacent chunks\n",
    ")\n",
    "\n",
    "# Load all PDF documents from the data directory\n",
    "path = \"/app/data\"\n",
    "loader = PyPDFDirectoryLoader(path)\n",
    "print(\"üìñ Loading PDF documents...\")\n",
    "docs_before_split = loader.load()\n",
    "print(f\"‚úÖ Loaded {len(docs_before_split)} document pages\")\n",
    "\n",
    "# Split documents into smaller, manageable chunks\n",
    "print(\"‚úÇÔ∏è Splitting documents into chunks...\")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "print(f\"‚úÖ Created {len(docs_after_split)} text chunks\")\n",
    "\n",
    "# Load the embedding model from local storage\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "model = sentence_transformers.SentenceTransformer(\"/app/data/model/\")\n",
    "print(\"‚úÖ Embedding model loaded\")\n",
    "\n",
    "# Process each document chunk: create embeddings and store in IRIS database\n",
    "print(\"üíæ Processing chunks and storing in database...\")\n",
    "for i, doc in enumerate(docs_after_split):\n",
    "    # Generate embeddings for the text content\n",
    "    # normalize_embeddings=True ensures consistent vector magnitudes for dot product similarity\n",
    "    embeddings = model.encode(doc.page_content, normalize_embeddings=True)\n",
    "    \n",
    "    # Convert to numpy array and format for database storage\n",
    "    array = np.array(embeddings)\n",
    "    formatted_array = np.vectorize('{:.12f}'.format)(array)  # 12 decimal precision\n",
    "    \n",
    "    # Prepare parameters for database insertion\n",
    "    parameters = [\n",
    "        doc.metadata['source'],                    # Source PDF file path\n",
    "        str(doc.page_content),                     # Actual text content\n",
    "        str(','.join(formatted_array))             # Comma-separated vector values\n",
    "    ]\n",
    "    \n",
    "    # Insert into IRIS database with vector storage\n",
    "    # TO_VECTOR() converts comma-separated string to IRIS vector format\n",
    "    cursorIRIS.execute(\n",
    "        \"INSERT INTO LLMRAG.DOCUMENTCHUNK (Document, Phrase, VectorizedPhrase) VALUES (?, ?, TO_VECTOR(?,DECIMAL))\", \n",
    "        parameters\n",
    "    )\n",
    "    \n",
    "    # Show progress every 10 chunks\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(docs_after_split)} chunks\")\n",
    "\n",
    "# Commit all changes to the database\n",
    "connectionIRIS.commit()\n",
    "print(f\"‚úÖ Successfully stored {len(docs_after_split)} chunks in InterSystems IRIS database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ur15ha8e9wr",
   "metadata": {},
   "source": [
    "## 5. Query Processing and Similarity Search\n",
    "\n",
    "Now we'll demonstrate how to query our vector database. We'll convert a question into an embedding and find the most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f4596f7-e4a6-464b-9086-91c5c16e6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing question: '¬øQu√© medicamento puede tomar mi hijo de 2 a√±os para bajar la fiebre?'\n",
      "üîç Searching for relevant documents...\n",
      "‚úÖ Found 1 relevant documents\n",
      "üìÑ /APP/DATA/PROSPECTO_69726.HTML.PDF (similarity: .6020914157820870586)\n"
     ]
    }
   ],
   "source": [
    "# Example question in Spanish (the model supports multiple languages)\n",
    "user_question = \"¬øQu√© medicamento puede tomar mi hijo de 2 a√±os para bajar la fiebre?\"\n",
    "\n",
    "print(f\"üîç Processing question: '{user_question}'\")\n",
    "\n",
    "# Convert the question into an embedding using the same model\n",
    "# This ensures semantic similarity with our stored document embeddings\n",
    "question_embedding = model.encode(user_question, normalize_embeddings=True)\n",
    "\n",
    "# Format the question embedding for database query\n",
    "array = np.array(question_embedding)\n",
    "formatted_array = np.vectorize('{:.12f}'.format)(array)\n",
    "parameter_query = [str(','.join(formatted_array))]\n",
    "\n",
    "# Perform similarity search in InterSystems IRIS\n",
    "# VECTOR_DOT_PRODUCT calculates similarity between question and document vectors\n",
    "# Similarity > 0.6 threshold filters for highly relevant documents\n",
    "print(\"üîç Searching for relevant documents...\")\n",
    "cursorIRIS.execute(\"\"\"\n",
    "    SELECT DISTINCT(Document), MAX(similarity) as max_similarity\n",
    "    FROM (\n",
    "        SELECT VECTOR_DOT_PRODUCT(VectorizedPhrase, TO_VECTOR(?, DECIMAL)) AS similarity, \n",
    "               Document \n",
    "        FROM LLMRAG.DOCUMENTCHUNK\n",
    "    ) \n",
    "    WHERE similarity > 0.6 \n",
    "    GROUP BY Document\n",
    "    ORDER BY max_similarity DESC\n",
    "\"\"\", parameter_query)\n",
    "\n",
    "similarity_rows = cursorIRIS.fetchall()\n",
    "print(f\"‚úÖ Found {len(similarity_rows)} relevant documents\")\n",
    "\n",
    "# Display the relevant documents and their similarity scores\n",
    "for doc_path, similarity in similarity_rows:\n",
    "    print(f\"üìÑ {doc_path} (similarity: {similarity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fzcyjcul21v",
   "metadata": {},
   "source": [
    "## 6. RAG Chain: Context Building and Answer Generation\n",
    "\n",
    "Now we'll build the context from relevant documents and use the LLM to generate an accurate, contextual answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "426e6299-8911-46a7-a83d-789b570434fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Building context from relevant documents...\n",
      "  Adding content from: /APP/DATA/PROSPECTO_69726.HTML.PDF\n",
      "‚úÖ Context built with 33004 characters\n",
      "üîß Loading RAG prompt template...\n",
      "‚öôÔ∏è Building RAG chain...\n",
      "ü§ñ Generating answer...\n",
      "==================================================\n",
      "Question: ¬øQu√© medicamento puede tomar mi hijo de 2 a√±os para bajar la fiebre?\n",
      "Answer: Para un ni√±o de **2 a√±os** (aproximadamente **10-12 kg**), puedes administrarle **Dalsy (ibuprofeno)** en una dosis de **1.8 a 2.4 mL por toma** (cada 6-8 horas), sin superar **7.2-9 mL al d√≠a** (288-360 mg/d√≠a). **Consulta siempre a un pediatra** antes de medicarlo, especialmente si la fiebre persiste m√°s de 24-48 horas.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Build context from relevant documents\n",
    "# We'll concatenate the full text of documents that matched our similarity search\n",
    "context = ''\n",
    "print(\"üìö Building context from relevant documents...\")\n",
    "\n",
    "for similarity_row in similarity_rows:\n",
    "    document_path = similarity_row[0]\n",
    "    print(f\"  Adding content from: {document_path}\")\n",
    "    \n",
    "    # Find the original document that matches this path\n",
    "    for doc in docs_before_split:\n",
    "        if similarity_row[0] == doc.metadata['source'].upper():\n",
    "            context += doc.page_content + \"\\n\\n\"  # Add spacing between documents\n",
    "\n",
    "print(f\"‚úÖ Context built with {len(context)} characters\")\n",
    "\n",
    "# Load a pre-built RAG prompt from LangChain Hub\n",
    "# This prompt template is optimized for question-answering with context\n",
    "print(\"üîß Loading RAG prompt template...\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Create the RAG chain using LangChain's pipeline syntax\n",
    "# This chain: 1) Passes context and question to prompt, 2) Sends to LLM, 3) Parses output\n",
    "print(\"‚öôÔ∏è Building RAG chain...\")\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: context,           # Provide the context we built\n",
    "        \"question\": RunnablePassthrough()       # Pass the question through unchanged\n",
    "    }\n",
    "    | prompt                                    # Apply the RAG prompt template\n",
    "    | llm                                      # Send to Mistral AI LLM\n",
    "    | StrOutputParser()                        # Parse the response as a string\n",
    ")\n",
    "\n",
    "# Generate the final answer\n",
    "print(\"ü§ñ Generating answer...\")\n",
    "print(\"=\" * 50)\n",
    "answer = rag_chain.invoke(user_question)\n",
    "print(f\"Question: {user_question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6l5a9h4l0du",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "Finally, let's properly close our database connection to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dae9ebab-79d7-41f4-af08-090f0ac22d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection closed successfully\n",
      "\n",
      "üéâ Workshop completed successfully!\n",
      "\n",
      "What we accomplished:\n",
      "‚úÖ Connected to InterSystems IRIS database\n",
      "‚úÖ Loaded and processed PDF documents\n",
      "‚úÖ Generated embeddings for semantic search\n",
      "‚úÖ Stored document chunks as vectors in IRIS\n",
      "‚úÖ Performed similarity search for relevant content\n",
      "‚úÖ Generated contextual answers using RAG pipeline\n",
      "\n",
      "üí° Try modifying the question above to test different queries!\n"
     ]
    }
   ],
   "source": [
    "# Close the database connection to free up resources\n",
    "connectionIRIS.close()\n",
    "print(\"‚úÖ Database connection closed successfully\")\n",
    "\n",
    "print(\"\\nüéâ Workshop completed successfully!\")\n",
    "print(\"\\nWhat we accomplished:\")\n",
    "print(\"‚úÖ Connected to InterSystems IRIS database\")\n",
    "print(\"‚úÖ Loaded and processed PDF documents\") \n",
    "print(\"‚úÖ Generated embeddings for semantic search\")\n",
    "print(\"‚úÖ Stored document chunks as vectors in IRIS\")\n",
    "print(\"‚úÖ Performed similarity search for relevant content\")\n",
    "print(\"‚úÖ Generated contextual answers using RAG pipeline\")\n",
    "print(\"\\nüí° Try modifying the question above to test different queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0ae9nlvc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Next Steps and Experiments\n",
    "\n",
    "Now that you have a working RAG system, try these enhancements:\n",
    "\n",
    "### 1. **Different Questions**\n",
    "Try asking questions in different languages or about different topics from your PDFs:\n",
    "```python\n",
    "# Example questions to try:\n",
    "questions = [\n",
    "    \"What are the side effects mentioned in the documents?\",\n",
    "    \"¬øCu√°l es la dosis recomendada para adultos?\",\n",
    "    \"How should this medication be stored?\",\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. **Adjust Similarity Threshold**\n",
    "Experiment with different similarity thresholds in the database query:\n",
    "- Higher threshold (0.8): More precise but fewer results\n",
    "- Lower threshold (0.4): More results but potentially less relevant\n",
    "\n",
    "### 3. **Different Embedding Models**\n",
    "Try other sentence transformer models:\n",
    "- `all-MiniLM-L6-v2`: Faster, English-focused\n",
    "- `all-mpnet-base-v2`: Higher quality embeddings\n",
    "- `multilingual-e5-large`: Better multilingual support\n",
    "\n",
    "### 4. **Chunk Size Optimization**\n",
    "Experiment with different `chunk_size` values:\n",
    "- Smaller chunks (300-500): More precise retrieval\n",
    "- Larger chunks (1000-1500): Better context preservation\n",
    "\n",
    "### 5. **Advanced Query Features**\n",
    "Add query expansion, keyword filtering, or hybrid search combining semantic and keyword matching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
