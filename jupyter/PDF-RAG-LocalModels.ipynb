{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dizxpt226b",
   "metadata": {},
   "source": [
    "# PDF Question Answering with Local Models and InterSystems IRIS\n",
    "\n",
    "This notebook demonstrates how to build a **fully local** Retrieval-Augmented Generation (RAG) system using:\n",
    "- **InterSystems IRIS** as the vector database\n",
    "- **Sentence Transformers** for text embeddings\n",
    "- **Local Transformer Models** for question answering (no external API needed)\n",
    "- **PyTorch** for local model execution\n",
    "\n",
    "## Key Advantages of Local QA\n",
    "✅ **Privacy**: All processing happens locally - no data sent to external APIs  \n",
    "✅ **Cost**: No API costs after initial setup  \n",
    "✅ **Speed**: Fast inference once models are loaded  \n",
    "✅ **Offline**: Works without internet connection  \n",
    "\n",
    "## Workshop Overview\n",
    "We'll process PDF documents, store them as vectors in IRIS, and enable natural language querying using entirely local models.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import all required libraries for our local RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7b5878-d158-4423-97d3-9baf3b4cfe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 All libraries imported successfully\n",
      "🔥 PyTorch using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for local RAG pipeline\n",
    "import irisnative                                  # InterSystems IRIS native database connection\n",
    "import os                                          # Operating system interface\n",
    "import sentence_transformers                       # Text embedding models\n",
    "import numpy as np                                 # Numerical computations\n",
    "\n",
    "# Document processing libraries\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Text chunking\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader # PDF loading\n",
    "\n",
    "# Local AI model libraries - these run entirely on your machine\n",
    "import torch                                       # PyTorch for deep learning\n",
    "from transformers import pipeline                  # Hugging Face transformers pipeline\n",
    "\n",
    "print(\"📦 All libraries imported successfully\")\n",
    "print(f\"🔥 PyTorch using device: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u740xiun2ci",
   "metadata": {},
   "source": [
    "## 2. Database Connection\n",
    "\n",
    "Let's establish our connection to the InterSystems IRIS database where we'll store our document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915cd2cb-059f-40b1-86b3-7ead09269b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to InterSystems IRIS database\n"
     ]
    }
   ],
   "source": [
    "# Database connection parameters\n",
    "# These should match your InterSystems IRIS instance configuration\n",
    "connection_string = \"iris:1972/LLMRAG\"  # host:port/namespace\n",
    "username = \"superuser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# Establish connection to InterSystems IRIS database\n",
    "# This creates both a connection and a cursor for executing SQL commands\n",
    "connectionIRIS = irisnative.createConnection(connection_string, username, password)\n",
    "cursorIRIS = connectionIRIS.cursor()\n",
    "print(\"✅ Connected to InterSystems IRIS database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tchagcpqmic",
   "metadata": {},
   "source": [
    "## 3. Local Question-Answering Model Setup\n",
    "\n",
    "We'll load a high-quality local QA model that can answer questions based on provided context. This model runs entirely on your machine without external API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18d9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading local question-answering model...\n",
      "   Model: mdeberta-v3-base-squad2 (optimized for multilingual QA)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b43bf7227a54a709626ea43bf0c78e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272aa95a283f44558127732b1e3d0977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015430eef3b84607a4b5a069abdbed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a603e793f014d25a251b5e197e91c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dd52be65a34a5f8b7fea3968d8bf3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb434bcc2de4f968f4962d6b82ab9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58813ac0ef74fdebece87bb0c2a3a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29611127b6c343459310f0e5bddff1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5dbefae34945e39cb167da4833e7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Local QA model loaded successfully\n",
      "💡 This model runs entirely offline - no internet required for inference!\n"
     ]
    }
   ],
   "source": [
    "# Load local question-answering model\n",
    "# timpal0l/mdeberta-v3-base-squad2 is a fine-tuned model excellent for extractive QA\n",
    "# It can find specific answers within provided text context\n",
    "print(\"📥 Loading local question-answering model...\")\n",
    "print(\"   Model: mdeberta-v3-base-squad2 (optimized for multilingual QA)\")\n",
    "\n",
    "qa_model = pipeline(\n",
    "    \"question-answering\", \n",
    "    \"timpal0l/mdeberta-v3-base-squad2\",\n",
    "    # Use GPU if available for faster inference\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"✅ Local QA model loaded successfully\")\n",
    "print(\"💡 This model runs entirely offline - no internet required for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8sb91r1khpa",
   "metadata": {},
   "source": [
    "## 4. Embedding Model Setup\n",
    "\n",
    "We need a sentence transformer model to convert text into numerical vectors (embeddings) for semantic similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3961787a-327b-4f8a-bd8e-a929ee686eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding model already available locally\n"
     ]
    }
   ],
   "source": [
    "# Check if the embedding model is already downloaded and saved locally\n",
    "# This saves time and bandwidth by avoiding re-downloads\n",
    "if not os.path.isdir('/app/data/model/'):\n",
    "    print(\"📥 Downloading and saving embedding model...\")\n",
    "    # paraphrase-multilingual-MiniLM-L12-v2 is excellent for multilingual semantic similarity\n",
    "    # It's lightweight but effective for most RAG applications\n",
    "    modelEmbedding = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    modelEmbedding.save('/app/data/model/')\n",
    "    print(\"✅ Embedding model saved to local directory\")\n",
    "else:\n",
    "    print(\"✅ Embedding model already available locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xldyzanillc",
   "metadata": {},
   "source": [
    "## 5. Document Processing and Vector Storage\n",
    "\n",
    "This is the core of our local RAG system: we'll load PDF documents, split them into chunks, create embeddings, and store everything in InterSystems IRIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944333c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Loading PDF documents...\n",
      "✅ Loaded 51 document pages\n",
      "✂️ Splitting documents into chunks...\n",
      "✅ Created 239 text chunks\n",
      "🔄 Loading embedding model...\n",
      "✅ Embedding model loaded\n",
      "💾 Processing chunks and storing in database...\n",
      "  Processed 10/239 chunks\n",
      "  Processed 20/239 chunks\n",
      "  Processed 30/239 chunks\n",
      "  Processed 40/239 chunks\n",
      "  Processed 50/239 chunks\n",
      "  Processed 60/239 chunks\n",
      "  Processed 70/239 chunks\n",
      "  Processed 80/239 chunks\n",
      "  Processed 90/239 chunks\n",
      "  Processed 100/239 chunks\n",
      "  Processed 110/239 chunks\n",
      "  Processed 120/239 chunks\n",
      "  Processed 130/239 chunks\n",
      "  Processed 140/239 chunks\n",
      "  Processed 150/239 chunks\n",
      "  Processed 160/239 chunks\n",
      "  Processed 170/239 chunks\n",
      "  Processed 180/239 chunks\n",
      "  Processed 190/239 chunks\n",
      "  Processed 200/239 chunks\n",
      "  Processed 210/239 chunks\n",
      "  Processed 220/239 chunks\n",
      "  Processed 230/239 chunks\n",
      "✅ Successfully stored 239 chunks in InterSystems IRIS database\n"
     ]
    }
   ],
   "source": [
    "# Configure text splitting strategy\n",
    "# Smaller chunks (700 chars) ensure focused, relevant context retrieval\n",
    "# Overlap (50 chars) prevents important information from being split across chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,      # Maximum characters per chunk\n",
    "    chunk_overlap=50,    # Characters to overlap between adjacent chunks\n",
    ")\n",
    "\n",
    "# Load all PDF documents from the data directory\n",
    "path = \"/app/data\"\n",
    "loader = PyPDFDirectoryLoader(path)\n",
    "print(\"📖 Loading PDF documents...\")\n",
    "docs_before_split = loader.load()\n",
    "print(f\"✅ Loaded {len(docs_before_split)} document pages\")\n",
    "\n",
    "# Split documents into smaller, manageable chunks\n",
    "print(\"✂️ Splitting documents into chunks...\")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "print(f\"✅ Created {len(docs_after_split)} text chunks\")\n",
    "\n",
    "# Load the embedding model from local storage\n",
    "print(\"🔄 Loading embedding model...\")\n",
    "modelEmbedding = sentence_transformers.SentenceTransformer(\"/app/data/model/\")\n",
    "print(\"✅ Embedding model loaded\")\n",
    "\n",
    "# Process each document chunk: create embeddings and store in IRIS database\n",
    "print(\"💾 Processing chunks and storing in database...\")\n",
    "for i, doc in enumerate(docs_after_split):\n",
    "    # Generate embeddings for the text content\n",
    "    # normalize_embeddings=True ensures consistent vector magnitudes for dot product similarity\n",
    "    embeddings = modelEmbedding.encode(doc.page_content, normalize_embeddings=True)\n",
    "    \n",
    "    # Convert to numpy array and format for database storage\n",
    "    array = np.array(embeddings)\n",
    "    formatted_array = np.vectorize('{:.12f}'.format)(array)  # 12 decimal precision\n",
    "    \n",
    "    # Prepare parameters for database insertion\n",
    "    parameters = [\n",
    "        doc.metadata['source'],                    # Source PDF file path\n",
    "        str(doc.page_content),                     # Actual text content\n",
    "        str(','.join(formatted_array))             # Comma-separated vector values\n",
    "    ]\n",
    "    \n",
    "    # Insert into IRIS database with vector storage\n",
    "    # TO_VECTOR() converts comma-separated string to IRIS vector format\n",
    "    cursorIRIS.execute(\n",
    "        \"INSERT INTO LLMRAG.DOCUMENTCHUNK (Document, Phrase, VectorizedPhrase) VALUES (?, ?, TO_VECTOR(?,DECIMAL))\", \n",
    "        parameters\n",
    "    )\n",
    "    \n",
    "    # Show progress every 10 chunks\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(docs_after_split)} chunks\")\n",
    "\n",
    "# Commit all changes to the database\n",
    "connectionIRIS.commit()\n",
    "print(f\"✅ Successfully stored {len(docs_after_split)} chunks in InterSystems IRIS database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ygj5lghqhet",
   "metadata": {},
   "source": [
    "## 6. Query Processing and Similarity Search\n",
    "\n",
    "Now we'll demonstrate how to query our vector database. We'll convert a question into an embedding and find the most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4596f7-e4a6-464b-9086-91c5c16e6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing question: '¿Qué medicamento puede tomar mi hijo de 2 años para bajar la fiebre?'\n",
      "🔍 Searching for relevant documents...\n",
      "✅ Found 1 relevant documents\n",
      "  📄 /APP/DATA/PROSPECTO_69726.HTML.PDF (similarity: .6020914157820870586)\n"
     ]
    }
   ],
   "source": [
    "# Example question in Spanish (the model supports multiple languages)\n",
    "literalQuestion = \"¿Qué medicamento puede tomar mi hijo de 2 años para bajar la fiebre?\"\n",
    "\n",
    "print(f\"🔍 Processing question: '{literalQuestion}'\")\n",
    "\n",
    "# Convert the question into an embedding using the same model\n",
    "# This ensures semantic similarity with our stored document embeddings\n",
    "question_embedding = modelEmbedding.encode(literalQuestion, normalize_embeddings=True)\n",
    "\n",
    "# Format the question embedding for database query\n",
    "array = np.array(question_embedding)\n",
    "formatted_array = np.vectorize('{:.12f}'.format)(array)\n",
    "parameter_query = [str(','.join(formatted_array))]\n",
    "\n",
    "# Perform similarity search in InterSystems IRIS\n",
    "# VECTOR_DOT_PRODUCT calculates similarity between question and document vectors\n",
    "# Similarity > 0.6 threshold filters for highly relevant documents\n",
    "print(\"🔍 Searching for relevant documents...\")\n",
    "cursorIRIS.execute(\"\"\"\n",
    "    SELECT DISTINCT(Document), MAX(similarity) as max_similarity\n",
    "    FROM (\n",
    "        SELECT VECTOR_DOT_PRODUCT(VectorizedPhrase, TO_VECTOR(?, DECIMAL)) AS similarity, \n",
    "               Document \n",
    "        FROM LLMRAG.DOCUMENTCHUNK\n",
    "    ) \n",
    "    WHERE similarity > 0.6 \n",
    "    GROUP BY Document\n",
    "    ORDER BY max_similarity DESC\n",
    "\"\"\", parameter_query)\n",
    "\n",
    "similarity_rows = cursorIRIS.fetchall()\n",
    "print(f\"✅ Found {len(similarity_rows)} relevant documents\")\n",
    "\n",
    "# Display the relevant documents and their similarity scores\n",
    "for doc_path, similarity in similarity_rows:\n",
    "    print(f\"  📄 {doc_path} (similarity: {similarity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yhbieyey7cr",
   "metadata": {},
   "source": [
    "### Debug: View Retrieved Documents\n",
    "\n",
    "Let's inspect the similarity search results to understand what documents were found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96f0539-cfa1-4049-b498-3a592d1b1c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Similarity search results:\n",
      "  1. Document: /APP/DATA/PROSPECTO_69726.HTML.PDF\n",
      "     Similarity: .6020914157820870586\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Document='/APP/DATA/PROSPECTO_69726.HTML.PDF', max_similarity='.6020914157820870586')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the similarity search results for debugging\n",
    "# This helps us understand which documents were retrieved and their relevance scores\n",
    "print(\"🔍 Similarity search results:\")\n",
    "for i, row in enumerate(similarity_rows, 1):\n",
    "    print(f\"  {i}. Document: {row[0]}\")\n",
    "    if len(row) > 1:  # If similarity score is available\n",
    "        print(f\"     Similarity: {row[1]}\")\n",
    "    print()\n",
    "\n",
    "similarity_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qyj3d6zum2q",
   "metadata": {},
   "source": [
    "## 7. Local Question Answering\n",
    "\n",
    "Now we'll use our local QA model to answer questions based on the retrieved context. This demonstrates the power of completely local AI processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426e6299-8911-46a7-a83d-789b570434fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Building context from relevant documents...\n",
      "  Adding content from: /APP/DATA/PROSPECTO_69726.HTML.PDF\n",
      "✅ Context built with 33004 characters\n",
      "============================================================\n",
      "🤖 LOCAL QUESTION ANSWERING DEMONSTRATION\n",
      "============================================================\n",
      "📝 Question: ¿Cómo se llama el medicamento descrito en el prospecto?\n",
      "📄 Context length: 33004 characters\n",
      "🧠 Using local model: mdeberta-v3-base-squad2\n",
      "\n",
      "🎯 ANSWER DETAILS:\n",
      "   Answer: \n",
      "Dalsy 40 mg/ml suspensión oral \n",
      "ibuprofeno\n",
      "   Confidence Score: 0.5234\n",
      "   Answer Position: characters 54-97\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.523392615839839,\n",
       " 'start': 54,\n",
       " 'end': 97,\n",
       " 'answer': '\\nDalsy 40 mg/ml suspensión oral \\nibuprofeno'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build context from relevant documents\n",
    "# We'll concatenate the full text of documents that matched our similarity search\n",
    "context = ''\n",
    "print(\"📚 Building context from relevant documents...\")\n",
    "\n",
    "for similarity_row in similarity_rows:\n",
    "    document_path = similarity_row[0]\n",
    "    print(f\"  Adding content from: {document_path}\")\n",
    "    \n",
    "    # Find the original document that matches this path\n",
    "    for doc in docs_before_split:\n",
    "        if similarity_row[0] == doc.metadata['source'].upper():\n",
    "            context += doc.page_content + \"\\n\\n\"  # Add spacing between documents\n",
    "\n",
    "print(f\"✅ Context built with {len(context)} characters\")\n",
    "\n",
    "# Example question about the medicine name\n",
    "example_question = \"¿Cómo se llama el medicamento descrito en el prospecto?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🤖 LOCAL QUESTION ANSWERING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"📝 Question: {example_question}\")\n",
    "print(f\"📄 Context length: {len(context)} characters\")\n",
    "print(f\"🧠 Using local model: mdeberta-v3-base-squad2\")\n",
    "print()\n",
    "\n",
    "# Use the local QA model to answer the question\n",
    "# This runs entirely on your machine - no external API calls!\n",
    "result = qa_model(question=example_question, context=context)\n",
    "\n",
    "print(\"🎯 ANSWER DETAILS:\")\n",
    "print(f\"   Answer: {result['answer']}\")\n",
    "print(f\"   Confidence Score: {result['score']:.4f}\")\n",
    "print(f\"   Answer Position: characters {result['start']}-{result['end']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display the result for inspection\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ki2zdvr6mxj",
   "metadata": {},
   "source": [
    "## 8. Interactive Question Testing\n",
    "\n",
    "Let's create a simple function to test different questions with our local RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdxyw08ptjh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING LOCAL QA SYSTEM WITH MULTIPLE QUESTIONS\n",
      "============================================================\n",
      "\n",
      "1. 🔍 Question: ¿Qué medicamento puede tomar mi hijo de 2 años para bajar la fiebre?\n",
      "   Found 1 relevant documents\n",
      "   Answer: \n",
      "ibuprofeno\n",
      "   Confidence: 0.07409860172720073\n",
      "\n",
      "   ✅ Successfully answered with 0.07409860172720073 confidence\n",
      "\n",
      "2. 🔍 Question: ¿Cuál es la dosis recomendada?\n",
      "   Found 4 relevant documents\n",
      "   Answer:  1 comprimido (25 mg) al día,\n",
      "   Confidence: 0.7944133747369051\n",
      "\n",
      "   ✅ Successfully answered with 0.7944133747369051 confidence\n",
      "\n",
      "3. 🔍 Question: ¿Cuáles son los efectos secundarios?\n",
      "   Found 3 relevant documents\n",
      "   Answer:  Posibles efectos adversos\n",
      "   Confidence: 0.7938088556693401\n",
      "\n",
      "   ✅ Successfully answered with 0.7938088556693401 confidence\n",
      "\n",
      "4. 🔍 Question: ¿Cómo se debe almacenar este medicamento?\n",
      "   Found 3 relevant documents\n",
      "   Answer:  en su envase original.\n",
      "   Confidence: 0.5308870549779385\n",
      "\n",
      "   ✅ Successfully answered with 0.5308870549779385 confidence\n",
      "\n",
      "💡 Try modifying the questions or similarity threshold to experiment!\n"
     ]
    }
   ],
   "source": [
    "def ask_local_question(question, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Ask a question to our local RAG system\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask\n",
    "        similarity_threshold (float): Minimum similarity score for document retrieval\n",
    "    \n",
    "    Returns:\n",
    "        dict: Answer with confidence score and metadata\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Question: {question}\")\n",
    "    \n",
    "    # Convert question to embedding\n",
    "    question_embedding = modelEmbedding.encode(question, normalize_embeddings=True)\n",
    "    array = np.array(question_embedding)\n",
    "    formatted_array = np.vectorize('{:.12f}'.format)(array)\n",
    "    parameter_query = [str(','.join(formatted_array))]\n",
    "    \n",
    "    # Search for relevant documents\n",
    "    cursorIRIS.execute(f\"\"\"\n",
    "        SELECT DISTINCT(Document), MAX(similarity) as max_similarity\n",
    "        FROM (\n",
    "            SELECT VECTOR_DOT_PRODUCT(VectorizedPhrase, TO_VECTOR(?, DECIMAL)) AS similarity, \n",
    "                   Document \n",
    "            FROM LLMRAG.DOCUMENTCHUNK\n",
    "        ) \n",
    "        WHERE similarity > {similarity_threshold}\n",
    "        GROUP BY Document\n",
    "        ORDER BY max_similarity DESC\n",
    "    \"\"\", parameter_query)\n",
    "    \n",
    "    results = cursorIRIS.fetchall()\n",
    "    print(f\"   Found {len(results)} relevant documents\")\n",
    "    \n",
    "    if not results:\n",
    "        return {\"error\": \"No relevant documents found. Try lowering the similarity threshold.\"}\n",
    "    \n",
    "    # Build context\n",
    "    context = ''\n",
    "    for result in results:\n",
    "        for doc in docs_before_split:\n",
    "            if result[0] == doc.metadata['source'].upper():\n",
    "                context += doc.page_content + \"\\n\\n\"\n",
    "    \n",
    "    # Get answer from local QA model\n",
    "    answer = qa_model(question=question, context=context)\n",
    "    \n",
    "    print(f\"   Answer: {answer['answer']}\")\n",
    "    print(f\"   Confidence: {answer['score']}\")\n",
    "    print()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"¿Qué medicamento puede tomar mi hijo de 2 años para bajar la fiebre?\",\n",
    "    \"¿Cuál es la dosis recomendada?\", \n",
    "    \"¿Cuáles son los efectos secundarios?\",\n",
    "    \"¿Cómo se debe almacenar este medicamento?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 TESTING LOCAL QA SYSTEM WITH MULTIPLE QUESTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. \", end=\"\")\n",
    "    result = ask_local_question(question)\n",
    "    if \"error\" not in result:\n",
    "        print(f\"   ✅ Successfully answered with {result['score']} confidence\")\n",
    "    else:\n",
    "        print(f\"   ❌ {result['error']}\")\n",
    "\n",
    "print(\"\\n💡 Try modifying the questions or similarity threshold to experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2n6io130p",
   "metadata": {},
   "source": [
    "## 9. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae9ebab-79d7-41f4-af08-090f0ac22d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database connection closed successfully\n",
      "\n",
      "🎉 Local RAG Workshop completed successfully!\n",
      "\n",
      "🏆 What we accomplished with LOCAL MODELS ONLY:\n",
      "✅ Connected to InterSystems IRIS vector database\n",
      "✅ Loaded PDF documents and created text chunks\n",
      "✅ Generated embeddings using local sentence transformers\n",
      "✅ Stored document vectors in IRIS for fast similarity search\n",
      "✅ Performed semantic search for relevant content\n",
      "✅ Generated answers using local transformer QA model\n",
      "✅ Created an interactive QA function for testing\n",
      "\n",
      "🔒 PRIVACY & PERFORMANCE BENEFITS:\n",
      "• No data sent to external APIs - complete privacy\n",
      "• No API costs - runs entirely on your infrastructure\n",
      "• Fast local inference after model loading\n",
      "• Works offline without internet connectivity\n",
      "• Full control over model selection and parameters\n",
      "\n",
      "💡 Next steps: Try different questions, adjust similarity thresholds, or experiment with other local models!\n"
     ]
    }
   ],
   "source": [
    "# Close the database connection to free up resources\n",
    "connectionIRIS.close()\n",
    "print(\"✅ Database connection closed successfully\")\n",
    "\n",
    "print(\"\\n🎉 Local RAG Workshop completed successfully!\")\n",
    "print(\"\\n🏆 What we accomplished with LOCAL MODELS ONLY:\")\n",
    "print(\"✅ Connected to InterSystems IRIS vector database\")\n",
    "print(\"✅ Loaded PDF documents and created text chunks\") \n",
    "print(\"✅ Generated embeddings using local sentence transformers\")\n",
    "print(\"✅ Stored document vectors in IRIS for fast similarity search\")\n",
    "print(\"✅ Performed semantic search for relevant content\")\n",
    "print(\"✅ Generated answers using local transformer QA model\")\n",
    "print(\"✅ Created an interactive QA function for testing\")\n",
    "\n",
    "print(\"\\n🔒 PRIVACY & PERFORMANCE BENEFITS:\")\n",
    "print(\"• No data sent to external APIs - complete privacy\")\n",
    "print(\"• No API costs - runs entirely on your infrastructure\")\n",
    "print(\"• Fast local inference after model loading\")\n",
    "print(\"• Works offline without internet connectivity\")\n",
    "print(\"• Full control over model selection and parameters\")\n",
    "\n",
    "print(\"\\n💡 Next steps: Try different questions, adjust similarity thresholds, or experiment with other local models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fox9owrvjwt",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 Advanced Experiments and Extensions\n",
    "\n",
    "### 1. **Model Comparisons**\n",
    "Try different local QA models by changing the pipeline initialization:\n",
    "```python\n",
    "# Alternative local QA models to experiment with:\n",
    "models_to_try = [\n",
    "    \"deepset/roberta-base-squad2\",              # English-focused, very fast\n",
    "    \"deepset/xlm-roberta-large-squad2\",         # Better multilingual support\n",
    "    \"microsoft/DialoGPT-medium\",                # Conversational responses\n",
    "    \"distilbert-base-cased-distilled-squad\"     # Lightweight and fast\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. **Performance Optimization**\n",
    "```python\n",
    "# Enable model optimization for production\n",
    "qa_model = pipeline(\n",
    "    \"question-answering\", \n",
    "    \"timpal0l/mdeberta-v3-base-squad2\",\n",
    "    device=0,  # Use GPU\n",
    "    torch_dtype=torch.float16,  # Half precision for speed\n",
    "    model_kwargs={\"low_cpu_mem_usage\": True}\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. **Hybrid Search Enhancement**\n",
    "Combine semantic and keyword search:\n",
    "```python\n",
    "# Add keyword matching to improve retrieval\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "```\n",
    "\n",
    "### 4. **Confidence Thresholds**\n",
    "Implement answer quality filtering:\n",
    "```python\n",
    "def get_confident_answer(question, min_confidence=0.3):\n",
    "    result = qa_model(question=question, context=context)\n",
    "    if result['score'] < min_confidence:\n",
    "        return \"I'm not confident enough to answer this question.\"\n",
    "    return result['answer']\n",
    "```\n",
    "\n",
    "### 5. **Multi-language Support**\n",
    "Test the system with questions in different languages:\n",
    "```python\n",
    "multilingual_questions = [\n",
    "    \"What is the recommended dosage?\",  # English\n",
    "    \"¿Cuál es la dosis recomendada?\",   # Spanish  \n",
    "    \"Quelle est la posologie recommandée?\",  # French\n",
    "    \"Qual è il dosaggio raccomandato?\"  # Italian\n",
    "]\n",
    "```\n",
    "\n",
    "### 📚 **Educational Notes**\n",
    "\n",
    "**Why Local Models?**\n",
    "- **Data Security**: Medical documents often contain sensitive information\n",
    "- **Compliance**: Meet GDPR, HIPAA, and other privacy requirements\n",
    "- **Cost Control**: No per-query API costs for high-volume usage\n",
    "- **Customization**: Fine-tune models on your specific domain data\n",
    "\n",
    "**Model Selection Guide:**\n",
    "- **Speed Priority**: DistilBERT-based models\n",
    "- **Accuracy Priority**: DeBERTa or RoBERTa-large models  \n",
    "- **Multilingual**: XLM-RoBERTa models\n",
    "- **Domain-specific**: Fine-tune on your documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
