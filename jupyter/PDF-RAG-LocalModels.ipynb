{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dizxpt226b",
   "metadata": {},
   "source": [
    "# PDF Question Answering with Local Models and InterSystems IRIS\n",
    "\n",
    "This notebook demonstrates how to build a **fully local** Retrieval-Augmented Generation (RAG) system using:\n",
    "- **InterSystems IRIS** as the vector database\n",
    "- **Sentence Transformers** for text embeddings\n",
    "- **Local Transformer Models** for question answering (no external API needed)\n",
    "- **PyTorch** for local model execution\n",
    "\n",
    "## Key Advantages of Local QA\n",
    "‚úÖ **Privacy**: All processing happens locally - no data sent to external APIs  \n",
    "‚úÖ **Cost**: No API costs after initial setup  \n",
    "‚úÖ **Speed**: Fast inference once models are loaded  \n",
    "‚úÖ **Offline**: Works without internet connection  \n",
    "\n",
    "## Workshop Overview\n",
    "We'll process PDF documents, store them as vectors in IRIS, and enable natural language querying using entirely local models.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import all required libraries for our local RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7b5878-d158-4423-97d3-9baf3b4cfe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ All libraries imported successfully\n",
      "üî• PyTorch using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for local RAG pipeline\n",
    "import irisnative                                  # InterSystems IRIS native database connection\n",
    "import os                                          # Operating system interface\n",
    "import sentence_transformers                       # Text embedding models\n",
    "import numpy as np                                 # Numerical computations\n",
    "\n",
    "# Document processing libraries\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Text chunking\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader # PDF loading\n",
    "\n",
    "# Local AI model libraries - these run entirely on your machine\n",
    "import torch                                       # PyTorch for deep learning\n",
    "from transformers import pipeline                  # Hugging Face transformers pipeline\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully\")\n",
    "print(f\"üî• PyTorch using device: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u740xiun2ci",
   "metadata": {},
   "source": [
    "## 2. Database Connection\n",
    "\n",
    "Let's establish our connection to the InterSystems IRIS database where we'll store our document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915cd2cb-059f-40b1-86b3-7ead09269b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to InterSystems IRIS database\n"
     ]
    }
   ],
   "source": [
    "# Database connection parameters\n",
    "# These should match your InterSystems IRIS instance configuration\n",
    "connection_string = \"iris:1972/LLMRAG\"  # host:port/namespace\n",
    "username = \"superuser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# Establish connection to InterSystems IRIS database\n",
    "# This creates both a connection and a cursor for executing SQL commands\n",
    "connectionIRIS = irisnative.createConnection(connection_string, username, password)\n",
    "cursorIRIS = connectionIRIS.cursor()\n",
    "print(\"‚úÖ Connected to InterSystems IRIS database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tchagcpqmic",
   "metadata": {},
   "source": [
    "## 3. Local Question-Answering Model Setup\n",
    "\n",
    "We'll load a high-quality local QA model that can answer questions based on provided context. This model runs entirely on your machine without external API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18d9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading local question-answering model...\n",
      "   Model: mdeberta-v3-base-squad2 (optimized for multilingual QA)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b43bf7227a54a709626ea43bf0c78e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/879 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272aa95a283f44558127732b1e3d0977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015430eef3b84607a4b5a069abdbed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a603e793f014d25a251b5e197e91c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dd52be65a34a5f8b7fea3968d8bf3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb434bcc2de4f968f4962d6b82ab9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58813ac0ef74fdebece87bb0c2a3a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29611127b6c343459310f0e5bddff1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5dbefae34945e39cb167da4833e7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local QA model loaded successfully\n",
      "üí° This model runs entirely offline - no internet required for inference!\n"
     ]
    }
   ],
   "source": [
    "# Load local question-answering model\n",
    "# timpal0l/mdeberta-v3-base-squad2 is a fine-tuned model excellent for extractive QA\n",
    "# It can find specific answers within provided text context\n",
    "print(\"üì• Loading local question-answering model...\")\n",
    "print(\"   Model: mdeberta-v3-base-squad2 (optimized for multilingual QA)\")\n",
    "\n",
    "qa_model = pipeline(\n",
    "    \"question-answering\", \n",
    "    \"timpal0l/mdeberta-v3-base-squad2\",\n",
    "    # Use GPU if available for faster inference\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Local QA model loaded successfully\")\n",
    "print(\"üí° This model runs entirely offline - no internet required for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8sb91r1khpa",
   "metadata": {},
   "source": [
    "## 4. Embedding Model Setup\n",
    "\n",
    "We need a sentence transformer model to convert text into numerical vectors (embeddings) for semantic similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3961787a-327b-4f8a-bd8e-a929ee686eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model already available locally\n"
     ]
    }
   ],
   "source": [
    "# Check if the embedding model is already downloaded and saved locally\n",
    "# This saves time and bandwidth by avoiding re-downloads\n",
    "if not os.path.isdir('/app/data/model/'):\n",
    "    print(\"üì• Downloading and saving embedding model...\")\n",
    "    # paraphrase-multilingual-MiniLM-L12-v2 is excellent for multilingual semantic similarity\n",
    "    # It's lightweight but effective for most RAG applications\n",
    "    modelEmbedding = sentence_transformers.SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    modelEmbedding.save('/app/data/model/')\n",
    "    print(\"‚úÖ Embedding model saved to local directory\")\n",
    "else:\n",
    "    print(\"‚úÖ Embedding model already available locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xldyzanillc",
   "metadata": {},
   "source": [
    "## 5. Document Processing and Vector Storage\n",
    "\n",
    "This is the core of our local RAG system: we'll load PDF documents, split them into chunks, create embeddings, and store everything in InterSystems IRIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944333c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading PDF documents...\n",
      "‚úÖ Loaded 51 document pages\n",
      "‚úÇÔ∏è Splitting documents into chunks...\n",
      "‚úÖ Created 239 text chunks\n",
      "üîÑ Loading embedding model...\n",
      "‚úÖ Embedding model loaded\n",
      "üíæ Processing chunks and storing in database...\n",
      "  Processed 10/239 chunks\n",
      "  Processed 20/239 chunks\n",
      "  Processed 30/239 chunks\n",
      "  Processed 40/239 chunks\n",
      "  Processed 50/239 chunks\n",
      "  Processed 60/239 chunks\n",
      "  Processed 70/239 chunks\n",
      "  Processed 80/239 chunks\n",
      "  Processed 90/239 chunks\n",
      "  Processed 100/239 chunks\n",
      "  Processed 110/239 chunks\n",
      "  Processed 120/239 chunks\n",
      "  Processed 130/239 chunks\n",
      "  Processed 140/239 chunks\n",
      "  Processed 150/239 chunks\n",
      "  Processed 160/239 chunks\n",
      "  Processed 170/239 chunks\n",
      "  Processed 180/239 chunks\n",
      "  Processed 190/239 chunks\n",
      "  Processed 200/239 chunks\n",
      "  Processed 210/239 chunks\n",
      "  Processed 220/239 chunks\n",
      "  Processed 230/239 chunks\n",
      "‚úÖ Successfully stored 239 chunks in InterSystems IRIS database\n"
     ]
    }
   ],
   "source": [
    "# Configure text splitting strategy\n",
    "# Smaller chunks (700 chars) ensure focused, relevant context retrieval\n",
    "# Overlap (50 chars) prevents important information from being split across chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,      # Maximum characters per chunk\n",
    "    chunk_overlap=50,    # Characters to overlap between adjacent chunks\n",
    ")\n",
    "\n",
    "# Load all PDF documents from the data directory\n",
    "path = \"/app/data\"\n",
    "loader = PyPDFDirectoryLoader(path)\n",
    "print(\"üìñ Loading PDF documents...\")\n",
    "docs_before_split = loader.load()\n",
    "print(f\"‚úÖ Loaded {len(docs_before_split)} document pages\")\n",
    "\n",
    "# Split documents into smaller, manageable chunks\n",
    "print(\"‚úÇÔ∏è Splitting documents into chunks...\")\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "print(f\"‚úÖ Created {len(docs_after_split)} text chunks\")\n",
    "\n",
    "# Load the embedding model from local storage\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "modelEmbedding = sentence_transformers.SentenceTransformer(\"/app/data/model/\")\n",
    "print(\"‚úÖ Embedding model loaded\")\n",
    "\n",
    "# Process each document chunk: create embeddings and store in IRIS database\n",
    "print(\"üíæ Processing chunks and storing in database...\")\n",
    "for i, doc in enumerate(docs_after_split):\n",
    "    # Generate embeddings for the text content\n",
    "    # normalize_embeddings=True ensures consistent vector magnitudes for dot product similarity\n",
    "    embeddings = modelEmbedding.encode(doc.page_content, normalize_embeddings=True)\n",
    "    \n",
    "    # Convert to numpy array and format for database storage\n",
    "    array = np.array(embeddings)\n",
    "    formatted_array = np.vectorize('{:.12f}'.format)(array)  # 12 decimal precision\n",
    "    \n",
    "    # Prepare parameters for database insertion\n",
    "    parameters = [\n",
    "        doc.metadata['source'],                    # Source PDF file path\n",
    "        str(doc.page_content),                     # Actual text content\n",
    "        str(','.join(formatted_array))             # Comma-separated vector values\n",
    "    ]\n",
    "    \n",
    "    # Insert into IRIS database with vector storage\n",
    "    # TO_VECTOR() converts comma-separated string to IRIS vector format\n",
    "    cursorIRIS.execute(\n",
    "        \"INSERT INTO LLMRAG.DOCUMENTCHUNK (Document, Phrase, VectorizedPhrase) VALUES (?, ?, TO_VECTOR(?,DECIMAL))\", \n",
    "        parameters\n",
    "    )\n",
    "    \n",
    "    # Show progress every 10 chunks\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(docs_after_split)} chunks\")\n",
    "\n",
    "# Commit all changes to the database\n",
    "connectionIRIS.commit()\n",
    "print(f\"‚úÖ Successfully stored {len(docs_after_split)} chunks in InterSystems IRIS database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ygj5lghqhet",
   "metadata": {},
   "source": [
    "## 6. Query Processing and Similarity Search\n",
    "\n",
    "Now we'll demonstrate how to query our vector database. We'll convert a question into an embedding and find the most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4596f7-e4a6-464b-9086-91c5c16e6093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing question: '¬øQu√© medicamento puede tomar mi hijo de 2 a√±os para bajar la fiebre?'\n",
      "üîç Searching for relevant documents...\n",
      "‚úÖ Found 1 relevant documents\n",
      "  üìÑ /APP/DATA/PROSPECTO_69726.HTML.PDF (similarity: .6020914157820870586)\n"
     ]
    }
   ],
   "source": [
    "# Example question in Spanish (the model supports multiple languages)\n",
    "literalQuestion = \"¬øQu√© medicamento puede tomar mi hijo de 2 a√±os para bajar la fiebre?\"\n",
    "\n",
    "print(f\"üîç Processing question: '{literalQuestion}'\")\n",
    "\n",
    "# Convert the question into an embedding using the same model\n",
    "# This ensures semantic similarity with our stored document embeddings\n",
    "question_embedding = modelEmbedding.encode(literalQuestion, normalize_embeddings=True)\n",
    "\n",
    "# Format the question embedding for database query\n",
    "array = np.array(question_embedding)\n",
    "formatted_array = np.vectorize('{:.12f}'.format)(array)\n",
    "parameter_query = [str(','.join(formatted_array))]\n",
    "\n",
    "# Perform similarity search in InterSystems IRIS\n",
    "# VECTOR_DOT_PRODUCT calculates similarity between question and document vectors\n",
    "# Similarity > 0.6 threshold filters for highly relevant documents\n",
    "print(\"üîç Searching for relevant documents...\")\n",
    "cursorIRIS.execute(\"\"\"\n",
    "    SELECT DISTINCT(Document), MAX(similarity) as max_similarity\n",
    "    FROM (\n",
    "        SELECT VECTOR_DOT_PRODUCT(VectorizedPhrase, TO_VECTOR(?, DECIMAL)) AS similarity, \n",
    "               Document \n",
    "        FROM LLMRAG.DOCUMENTCHUNK\n",
    "    ) \n",
    "    WHERE similarity > 0.6 \n",
    "    GROUP BY Document\n",
    "    ORDER BY max_similarity DESC\n",
    "\"\"\", parameter_query)\n",
    "\n",
    "similarity_rows = cursorIRIS.fetchall()\n",
    "print(f\"‚úÖ Found {len(similarity_rows)} relevant documents\")\n",
    "\n",
    "# Display the relevant documents and their similarity scores\n",
    "for doc_path, similarity in similarity_rows:\n",
    "    print(f\"  üìÑ {doc_path} (similarity: {similarity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yhbieyey7cr",
   "metadata": {},
   "source": [
    "### Debug: View Retrieved Documents\n",
    "\n",
    "Let's inspect the similarity search results to understand what documents were found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96f0539-cfa1-4049-b498-3a592d1b1c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Similarity search results:\n",
      "  1. Document: /APP/DATA/PROSPECTO_69726.HTML.PDF\n",
      "     Similarity: .6020914157820870586\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Document='/APP/DATA/PROSPECTO_69726.HTML.PDF', max_similarity='.6020914157820870586')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the similarity search results for debugging\n",
    "# This helps us understand which documents were retrieved and their relevance scores\n",
    "print(\"üîç Similarity search results:\")\n",
    "for i, row in enumerate(similarity_rows, 1):\n",
    "    print(f\"  {i}. Document: {row[0]}\")\n",
    "    if len(row) > 1:  # If similarity score is available\n",
    "        print(f\"     Similarity: {row[1]}\")\n",
    "    print()\n",
    "\n",
    "similarity_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qyj3d6zum2q",
   "metadata": {},
   "source": [
    "## 7. Local Question Answering\n",
    "\n",
    "Now we'll use our local QA model to answer questions based on the retrieved context. This demonstrates the power of completely local AI processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426e6299-8911-46a7-a83d-789b570434fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Building context from relevant documents...\n",
      "  Adding content from: /APP/DATA/PROSPECTO_69726.HTML.PDF\n",
      "‚úÖ Context built with 33004 characters\n",
      "============================================================\n",
      "ü§ñ LOCAL QUESTION ANSWERING DEMONSTRATION\n",
      "============================================================\n",
      "üìù Question: ¬øC√≥mo se llama el medicamento descrito en el prospecto?\n",
      "üìÑ Context length: 33004 characters\n",
      "üß† Using local model: mdeberta-v3-base-squad2\n",
      "\n",
      "üéØ ANSWER DETAILS:\n",
      "   Answer: \n",
      "Dalsy 40 mg/ml suspensi√≥n oral \n",
      "ibuprofeno\n",
      "   Confidence Score: 0.5234\n",
      "   Answer Position: characters 54-97\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.523392615839839,\n",
       " 'start': 54,\n",
       " 'end': 97,\n",
       " 'answer': '\\nDalsy 40 mg/ml suspensi√≥n oral \\nibuprofeno'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build context from relevant documents\n",
    "# We'll concatenate the full text of documents that matched our similarity search\n",
    "context = ''\n",
    "print(\"üìö Building context from relevant documents...\")\n",
    "\n",
    "for similarity_row in similarity_rows:\n",
    "    document_path = similarity_row[0]\n",
    "    print(f\"  Adding content from: {document_path}\")\n",
    "    \n",
    "    # Find the original document that matches this path\n",
    "    for doc in docs_before_split:\n",
    "        if similarity_row[0] == doc.metadata['source'].upper():\n",
    "            context += doc.page_content + \"\\n\\n\"  # Add spacing between documents\n",
    "\n",
    "print(f\"‚úÖ Context built with {len(context)} characters\")\n",
    "\n",
    "# Example question about the medicine name\n",
    "example_question = \"¬øC√≥mo se llama el medicamento descrito en el prospecto?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ü§ñ LOCAL QUESTION ANSWERING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìù Question: {example_question}\")\n",
    "print(f\"üìÑ Context length: {len(context)} characters\")\n",
    "print(f\"üß† Using local model: mdeberta-v3-base-squad2\")\n",
    "print()\n",
    "\n",
    "# Use the local QA model to answer the question\n",
    "# This runs entirely on your machine - no external API calls!\n",
    "result = qa_model(question=example_question, context=context)\n",
    "\n",
    "print(\"üéØ ANSWER DETAILS:\")\n",
    "print(f\"   Answer: {result['answer']}\")\n",
    "print(f\"   Confidence Score: {result['score']:.4f}\")\n",
    "print(f\"   Answer Position: characters {result['start']}-{result['end']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display the result for inspection\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ki2zdvr6mxj",
   "metadata": {},
   "source": [
    "## 8. Interactive Question Testing\n",
    "\n",
    "Let's create a simple function to test different questions with our local RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdxyw08ptjh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING LOCAL QA SYSTEM WITH MULTIPLE QUESTIONS\n",
      "============================================================\n",
      "\n",
      "1. üîç Question: ¬øQu√© medicamento puede tomar mi hijo de 2 a√±os para bajar la fiebre?\n",
      "   Found 1 relevant documents\n",
      "   Answer: \n",
      "ibuprofeno\n",
      "   Confidence: 0.07409860172720073\n",
      "\n",
      "   ‚úÖ Successfully answered with 0.07409860172720073 confidence\n",
      "\n",
      "2. üîç Question: ¬øCu√°l es la dosis recomendada?\n",
      "   Found 4 relevant documents\n",
      "   Answer:  1 comprimido (25 mg) al d√≠a,\n",
      "   Confidence: 0.7944133747369051\n",
      "\n",
      "   ‚úÖ Successfully answered with 0.7944133747369051 confidence\n",
      "\n",
      "3. üîç Question: ¬øCu√°les son los efectos secundarios?\n",
      "   Found 3 relevant documents\n",
      "   Answer:  Posibles efectos adversos\n",
      "   Confidence: 0.7938088556693401\n",
      "\n",
      "   ‚úÖ Successfully answered with 0.7938088556693401 confidence\n",
      "\n",
      "4. üîç Question: ¬øC√≥mo se debe almacenar este medicamento?\n",
      "   Found 3 relevant documents\n",
      "   Answer:  en su envase original.\n",
      "   Confidence: 0.5308870549779385\n",
      "\n",
      "   ‚úÖ Successfully answered with 0.5308870549779385 confidence\n",
      "\n",
      "üí° Try modifying the questions or similarity threshold to experiment!\n"
     ]
    }
   ],
   "source": [
    "def ask_local_question(question, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Ask a question to our local RAG system\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask\n",
    "        similarity_threshold (float): Minimum similarity score for document retrieval\n",
    "    \n",
    "    Returns:\n",
    "        dict: Answer with confidence score and metadata\n",
    "    \"\"\"\n",
    "    print(f\"üîç Question: {question}\")\n",
    "    \n",
    "    # Convert question to embedding\n",
    "    question_embedding = modelEmbedding.encode(question, normalize_embeddings=True)\n",
    "    array = np.array(question_embedding)\n",
    "    formatted_array = np.vectorize('{:.12f}'.format)(array)\n",
    "    parameter_query = [str(','.join(formatted_array))]\n",
    "    \n",
    "    # Search for relevant documents\n",
    "    cursorIRIS.execute(f\"\"\"\n",
    "        SELECT DISTINCT(Document), MAX(similarity) as max_similarity\n",
    "        FROM (\n",
    "            SELECT VECTOR_DOT_PRODUCT(VectorizedPhrase, TO_VECTOR(?, DECIMAL)) AS similarity, \n",
    "                   Document \n",
    "            FROM LLMRAG.DOCUMENTCHUNK\n",
    "        ) \n",
    "        WHERE similarity > {similarity_threshold}\n",
    "        GROUP BY Document\n",
    "        ORDER BY max_similarity DESC\n",
    "    \"\"\", parameter_query)\n",
    "    \n",
    "    results = cursorIRIS.fetchall()\n",
    "    print(f\"   Found {len(results)} relevant documents\")\n",
    "    \n",
    "    if not results:\n",
    "        return {\"error\": \"No relevant documents found. Try lowering the similarity threshold.\"}\n",
    "    \n",
    "    # Build context\n",
    "    context = ''\n",
    "    for result in results:\n",
    "        for doc in docs_before_split:\n",
    "            if result[0] == doc.metadata['source'].upper():\n",
    "                context += doc.page_content + \"\\n\\n\"\n",
    "    \n",
    "    # Get answer from local QA model\n",
    "    answer = qa_model(question=question, context=context)\n",
    "    \n",
    "    print(f\"   Answer: {answer['answer']}\")\n",
    "    print(f\"   Confidence: {answer['score']}\")\n",
    "    print()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"¬øQu√© medicamento puede tomar mi hijo de 2 a√±os para bajar la fiebre?\",\n",
    "    \"¬øCu√°l es la dosis recomendada?\", \n",
    "    \"¬øCu√°les son los efectos secundarios?\",\n",
    "    \"¬øC√≥mo se debe almacenar este medicamento?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING LOCAL QA SYSTEM WITH MULTIPLE QUESTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. \", end=\"\")\n",
    "    result = ask_local_question(question)\n",
    "    if \"error\" not in result:\n",
    "        print(f\"   ‚úÖ Successfully answered with {result['score']} confidence\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {result['error']}\")\n",
    "\n",
    "print(\"\\nüí° Try modifying the questions or similarity threshold to experiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2n6io130p",
   "metadata": {},
   "source": [
    "## 9. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae9ebab-79d7-41f4-af08-090f0ac22d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection closed successfully\n",
      "\n",
      "üéâ Local RAG Workshop completed successfully!\n",
      "\n",
      "üèÜ What we accomplished with LOCAL MODELS ONLY:\n",
      "‚úÖ Connected to InterSystems IRIS vector database\n",
      "‚úÖ Loaded PDF documents and created text chunks\n",
      "‚úÖ Generated embeddings using local sentence transformers\n",
      "‚úÖ Stored document vectors in IRIS for fast similarity search\n",
      "‚úÖ Performed semantic search for relevant content\n",
      "‚úÖ Generated answers using local transformer QA model\n",
      "‚úÖ Created an interactive QA function for testing\n",
      "\n",
      "üîí PRIVACY & PERFORMANCE BENEFITS:\n",
      "‚Ä¢ No data sent to external APIs - complete privacy\n",
      "‚Ä¢ No API costs - runs entirely on your infrastructure\n",
      "‚Ä¢ Fast local inference after model loading\n",
      "‚Ä¢ Works offline without internet connectivity\n",
      "‚Ä¢ Full control over model selection and parameters\n",
      "\n",
      "üí° Next steps: Try different questions, adjust similarity thresholds, or experiment with other local models!\n"
     ]
    }
   ],
   "source": [
    "# Close the database connection to free up resources\n",
    "connectionIRIS.close()\n",
    "print(\"‚úÖ Database connection closed successfully\")\n",
    "\n",
    "print(\"\\nüéâ Local RAG Workshop completed successfully!\")\n",
    "print(\"\\nüèÜ What we accomplished with LOCAL MODELS ONLY:\")\n",
    "print(\"‚úÖ Connected to InterSystems IRIS vector database\")\n",
    "print(\"‚úÖ Loaded PDF documents and created text chunks\") \n",
    "print(\"‚úÖ Generated embeddings using local sentence transformers\")\n",
    "print(\"‚úÖ Stored document vectors in IRIS for fast similarity search\")\n",
    "print(\"‚úÖ Performed semantic search for relevant content\")\n",
    "print(\"‚úÖ Generated answers using local transformer QA model\")\n",
    "print(\"‚úÖ Created an interactive QA function for testing\")\n",
    "\n",
    "print(\"\\nüîí PRIVACY & PERFORMANCE BENEFITS:\")\n",
    "print(\"‚Ä¢ No data sent to external APIs - complete privacy\")\n",
    "print(\"‚Ä¢ No API costs - runs entirely on your infrastructure\")\n",
    "print(\"‚Ä¢ Fast local inference after model loading\")\n",
    "print(\"‚Ä¢ Works offline without internet connectivity\")\n",
    "print(\"‚Ä¢ Full control over model selection and parameters\")\n",
    "\n",
    "print(\"\\nüí° Next steps: Try different questions, adjust similarity thresholds, or experiment with other local models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fox9owrvjwt",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Advanced Experiments and Extensions\n",
    "\n",
    "### 1. **Model Comparisons**\n",
    "Try different local QA models by changing the pipeline initialization:\n",
    "```python\n",
    "# Alternative local QA models to experiment with:\n",
    "models_to_try = [\n",
    "    \"deepset/roberta-base-squad2\",              # English-focused, very fast\n",
    "    \"deepset/xlm-roberta-large-squad2\",         # Better multilingual support\n",
    "    \"microsoft/DialoGPT-medium\",                # Conversational responses\n",
    "    \"distilbert-base-cased-distilled-squad\"     # Lightweight and fast\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. **Performance Optimization**\n",
    "```python\n",
    "# Enable model optimization for production\n",
    "qa_model = pipeline(\n",
    "    \"question-answering\", \n",
    "    \"timpal0l/mdeberta-v3-base-squad2\",\n",
    "    device=0,  # Use GPU\n",
    "    torch_dtype=torch.float16,  # Half precision for speed\n",
    "    model_kwargs={\"low_cpu_mem_usage\": True}\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. **Hybrid Search Enhancement**\n",
    "Combine semantic and keyword search:\n",
    "```python\n",
    "# Add keyword matching to improve retrieval\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "```\n",
    "\n",
    "### 4. **Confidence Thresholds**\n",
    "Implement answer quality filtering:\n",
    "```python\n",
    "def get_confident_answer(question, min_confidence=0.3):\n",
    "    result = qa_model(question=question, context=context)\n",
    "    if result['score'] < min_confidence:\n",
    "        return \"I'm not confident enough to answer this question.\"\n",
    "    return result['answer']\n",
    "```\n",
    "\n",
    "### 5. **Multi-language Support**\n",
    "Test the system with questions in different languages:\n",
    "```python\n",
    "multilingual_questions = [\n",
    "    \"What is the recommended dosage?\",  # English\n",
    "    \"¬øCu√°l es la dosis recomendada?\",   # Spanish  \n",
    "    \"Quelle est la posologie recommand√©e?\",  # French\n",
    "    \"Qual √® il dosaggio raccomandato?\"  # Italian\n",
    "]\n",
    "```\n",
    "\n",
    "### üìö **Educational Notes**\n",
    "\n",
    "**Why Local Models?**\n",
    "- **Data Security**: Medical documents often contain sensitive information\n",
    "- **Compliance**: Meet GDPR, HIPAA, and other privacy requirements\n",
    "- **Cost Control**: No per-query API costs for high-volume usage\n",
    "- **Customization**: Fine-tune models on your specific domain data\n",
    "\n",
    "**Model Selection Guide:**\n",
    "- **Speed Priority**: DistilBERT-based models\n",
    "- **Accuracy Priority**: DeBERTa or RoBERTa-large models  \n",
    "- **Multilingual**: XLM-RoBERTa models\n",
    "- **Domain-specific**: Fine-tune on your documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
